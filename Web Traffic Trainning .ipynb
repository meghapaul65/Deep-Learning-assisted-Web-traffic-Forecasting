{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import unicode_literals\n\nimport os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n\ndef parse_page(x):\n    x = x.split('_')\n    return ' '.join(x[:-3]), x[-3], x[-2], x[-1]\n\n\ndef nan_fill_forward(x):\n    for i in range(x.shape[0]):\n        fill_val = None\n        for j in range(x.shape[1] - 3, x.shape[1]):\n            if np.isnan(x[i, j]) and fill_val is not None:\n                x[i, j] = fill_val\n            else:\n                fill_val = x[i, j]\n    return x\n\n\ndf = pd.read_csv('train_2.csv', encoding='utf-8')\ndate_cols = [i for i in df.columns if i != 'Page']\n\ndf['name'], df['project'], df['access'], df['agent'] = zip(*df['Page'].apply(parse_page))\n\nle = LabelEncoder()\ndf['project'] = le.fit_transform(df['project'])\ndf['access'] = le.fit_transform(df['access'])\ndf['agent'] = le.fit_transform(df['agent'])\ndf['page_id'] = le.fit_transform(df['Page'])\n\nif not os.path.isdir('data/processed'):\n    os.makedirs('data/processed')\n\ndf[['page_id', 'Page']].to_csv('data/processed/page_ids.csv', encoding='utf-8', index=False)\n\ndata = df[date_cols].values\nnp.save('data/processed/data.npy', np.nan_to_num(data))\nnp.save('data/processed/is_nan.npy', np.isnan(data).astype(int))\nnp.save('data/processed/project.npy', df['project'].values)\nnp.save('data/processed/access.npy', df['access'].values)\nnp.save('data/processed/agent.npy', df['agent'].values)\nnp.save('data/processed/page_id.npy', df['page_id'].values)\n\ntest_data = nan_fill_forward(df[date_cols].values)\nnp.save('data/processed/test_data.npy', np.nan_to_num(test_data))\nnp.save('data/processed/test_is_nan.npy', np.isnan(test_data).astype(int))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom data_frame import DataFrame\nfrom tf_base_model import TFBaseModel\nfrom tf_utils import (\n    time_distributed_dense_layer, temporal_convolution_layer,\n    sequence_mean, sequence_smape, shape\n)\n\n\nclass DataReader(object):\n\n    def __init__(self, data_dir):\n        data_cols = [\n            'data',\n            'is_nan',\n            'page_id',\n            'project',\n            'access',\n            'agent',\n            'test_data',\n            'test_is_nan'\n        ]\n        data = [np.load(os.path.join(data_dir, '{}.npy'.format(i))) for i in data_cols]\n\n        self.test_df = DataFrame(columns=data_cols, data=data)\n        self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.95)\n\n        print ('train size', len(self.train_df))\n        print ('val size', len(self.val_df))\n        print ('test size', len(self.test_df))\n\n    def train_batch_generator(self, batch_size):\n        return self.batch_generator(\n            batch_size=batch_size,\n            df=self.train_df,\n            shuffle=True,\n            num_epochs=10000,\n            is_test=False\n        )\n\n    def val_batch_generator(self, batch_size):\n        return self.batch_generator(\n            batch_size=batch_size,\n            df=self.val_df,\n            shuffle=True,\n            num_epochs=10000,\n            is_test=False\n        )\n\n    def test_batch_generator(self, batch_size):\n        return self.batch_generator(\n            batch_size=batch_size,\n            df=self.test_df,\n            shuffle=True,\n            num_epochs=1,\n            is_test=True\n        )\n\n    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n        batch_gen = df.batch_generator(\n            batch_size=batch_size,\n            shuffle=shuffle,\n            num_epochs=num_epochs,\n            allow_smaller_final_batch=is_test\n        )\n        data_col = 'test_data' if is_test else 'data'\n        is_nan_col = 'test_is_nan' if is_test else 'is_nan'\n        for batch in batch_gen:\n            num_decode_steps = 64\n            full_seq_len = batch[data_col].shape[1]\n            max_encode_length = full_seq_len - num_decode_steps if not is_test else full_seq_len\n\n            x_encode = np.zeros([len(batch), max_encode_length])\n            y_decode = np.zeros([len(batch), num_decode_steps])\n            is_nan_encode = np.zeros([len(batch), max_encode_length])\n            is_nan_decode = np.zeros([len(batch), num_decode_steps])\n            encode_len = np.zeros([len(batch)])\n            decode_len = np.zeros([len(batch)])\n\n            for i, (seq, nan_seq) in enumerate(zip(batch[data_col], batch[is_nan_col])):\n                rand_len = np.random.randint(max_encode_length - 365 + 1, max_encode_length + 1)\n                x_encode_len = max_encode_length if is_test else rand_len\n                x_encode[i, :x_encode_len] = seq[:x_encode_len]\n                is_nan_encode[i, :x_encode_len] = nan_seq[:x_encode_len]\n                encode_len[i] = x_encode_len\n                decode_len[i] = num_decode_steps\n                if not is_test:\n                    y_decode[i, :] = seq[x_encode_len: x_encode_len + num_decode_steps]\n                    is_nan_decode[i, :] = nan_seq[x_encode_len: x_encode_len + num_decode_steps]\n\n            batch['x_encode'] = x_encode\n            batch['encode_len'] = encode_len\n            batch['y_decode'] = y_decode\n            batch['decode_len'] = decode_len\n            batch['is_nan_encode'] = is_nan_encode\n            batch['is_nan_decode'] = is_nan_decode\n\n            yield batch\n\n\nclass cnn(TFBaseModel):\n\n    def __init__(\n        self,\n        residual_channels=32,\n        skip_channels=32,\n        dilations=[2**i for i in range(8)]*3,\n        filter_widths=[2 for i in range(8)]*3,\n        num_decode_steps=64,\n        **kwargs\n    ):\n        self.residual_channels = residual_channels\n        self.skip_channels = skip_channels\n        self.dilations = dilations\n        self.filter_widths = filter_widths\n        self.num_decode_steps = num_decode_steps\n        super(cnn, self).__init__(**kwargs)\n\n    def transform(self, x):\n        return tf.log(x + 1) - tf.expand_dims(self.log_x_encode_mean, 1)\n\n    def inverse_transform(self, x):\n        return tf.exp(x + tf.expand_dims(self.log_x_encode_mean, 1)) - 1\n\n    def get_input_sequences(self):\n        self.x_encode = tf.placeholder(tf.float32, [None, None])\n        self.encode_len = tf.placeholder(tf.int32, [None])\n        self.y_decode = tf.placeholder(tf.float32, [None, self.num_decode_steps])\n        self.decode_len = tf.placeholder(tf.int32, [None])\n        self.is_nan_encode = tf.placeholder(tf.float32, [None, None])\n        self.is_nan_decode = tf.placeholder(tf.float32, [None, self.num_decode_steps])\n\n        self.page_id = tf.placeholder(tf.int32, [None])\n        self.project = tf.placeholder(tf.int32, [None])\n        self.access = tf.placeholder(tf.int32, [None])\n        self.agent = tf.placeholder(tf.int32, [None])\n\n        self.keep_prob = tf.placeholder(tf.float32)\n        self.is_training = tf.placeholder(tf.bool)\n\n        self.log_x_encode_mean = sequence_mean(tf.log(self.x_encode + 1), self.encode_len)\n        self.log_x_encode = self.transform(self.x_encode)\n        self.x = tf.expand_dims(self.log_x_encode, 2)\n\n        self.encode_features = tf.concat([\n            tf.expand_dims(self.is_nan_encode, 2),\n            tf.expand_dims(tf.cast(tf.equal(self.x_encode, 0.0), tf.float32), 2),\n            tf.tile(tf.reshape(self.log_x_encode_mean, (-1, 1, 1)), (1, tf.shape(self.x_encode)[1], 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.project, 9), 1), (1, tf.shape(self.x_encode)[1], 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.access, 3), 1), (1, tf.shape(self.x_encode)[1], 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.agent, 2), 1), (1, tf.shape(self.x_encode)[1], 1)),\n        ], axis=2)\n\n        decode_idx = tf.tile(tf.expand_dims(tf.range(self.num_decode_steps), 0), (tf.shape(self.y_decode)[0], 1))\n        self.decode_features = tf.concat([\n            tf.one_hot(decode_idx, self.num_decode_steps),\n            tf.tile(tf.reshape(self.log_x_encode_mean, (-1, 1, 1)), (1, self.num_decode_steps, 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.project, 9), 1), (1, self.num_decode_steps, 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.access, 3), 1), (1, self.num_decode_steps, 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.agent, 2), 1), (1, self.num_decode_steps, 1)),\n        ], axis=2)\n\n        return self.x\n\n    def encode(self, x, features):\n        x = tf.concat([x, features], axis=2)\n\n        inputs = time_distributed_dense_layer(\n            inputs=x,\n            output_units=self.residual_channels,\n            activation=tf.nn.tanh,\n            scope='x-proj-encode'\n        )\n\n        skip_outputs = []\n        conv_inputs = [inputs]\n        for i, (dilation, filter_width) in enumerate(zip(self.dilations, self.filter_widths)):\n            dilated_conv = temporal_convolution_layer(\n                inputs=inputs,\n                output_units=2*self.residual_channels,\n                convolution_width=filter_width,\n                causal=True,\n                dilation_rate=[dilation],\n                scope='dilated-conv-encode-{}'.format(i)\n            )\n            conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=2)\n            dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n\n            outputs = time_distributed_dense_layer(\n                inputs=dilated_conv,\n                output_units=self.skip_channels + self.residual_channels,\n                scope='dilated-conv-proj-encode-{}'.format(i)\n            )\n            skips, residuals = tf.split(outputs, [self.skip_channels, self.residual_channels], axis=2)\n\n            inputs += residuals\n            conv_inputs.append(inputs)\n            skip_outputs.append(skips)\n\n        skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=2))\n        h = time_distributed_dense_layer(skip_outputs, 128, scope='dense-encode-1', activation=tf.nn.relu)\n        y_hat = time_distributed_dense_layer(h, 1, scope='dense-encode-2')\n\n        return y_hat, conv_inputs[:-1]\n\n    def initialize_decode_params(self, x, features):\n        x = tf.concat([x, features], axis=2)\n\n        inputs = time_distributed_dense_layer(\n            inputs=x,\n            output_units=self.residual_channels,\n            activation=tf.nn.tanh,\n            scope='x-proj-decode'\n        )\n\n        skip_outputs = []\n        conv_inputs = [inputs]\n        for i, (dilation, filter_width) in enumerate(zip(self.dilations, self.filter_widths)):\n            dilated_conv = temporal_convolution_layer(\n                inputs=inputs,\n                output_units=2*self.residual_channels,\n                convolution_width=filter_width,\n                causal=True,\n                dilation_rate=[dilation],\n                scope='dilated-conv-decode-{}'.format(i)\n            )\n            conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=2)\n            dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n\n            outputs = time_distributed_dense_layer(\n                inputs=dilated_conv,\n                output_units=self.skip_channels + self.residual_channels,\n                scope='dilated-conv-proj-decode-{}'.format(i)\n            )\n            skips, residuals = tf.split(outputs, [self.skip_channels, self.residual_channels], axis=2)\n\n            inputs += residuals\n            conv_inputs.append(inputs)\n            skip_outputs.append(skips)\n\n        skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=2))\n        h = time_distributed_dense_layer(skip_outputs, 128, scope='dense-decode-1', activation=tf.nn.relu)\n        y_hat = time_distributed_dense_layer(h, 1, scope='dense-decode-2')\n        return y_hat\n\n    def decode(self, x, conv_inputs, features):\n        batch_size = tf.shape(x)[0]\n\n        # initialize state tensor arrays\n        state_queues = []\n        for i, (conv_input, dilation) in enumerate(zip(conv_inputs, self.dilations)):\n            batch_idx = tf.range(batch_size)\n            batch_idx = tf.tile(tf.expand_dims(batch_idx, 1), (1, dilation))\n            batch_idx = tf.reshape(batch_idx, [-1])\n\n            queue_begin_time = self.encode_len - dilation - 1\n            temporal_idx = tf.expand_dims(queue_begin_time, 1) + tf.expand_dims(tf.range(dilation), 0)\n            temporal_idx = tf.reshape(temporal_idx, [-1])\n\n            idx = tf.stack([batch_idx, temporal_idx], axis=1)\n            slices = tf.reshape(tf.gather_nd(conv_input, idx), (batch_size, dilation, shape(conv_input, 2)))\n\n            layer_ta = tf.TensorArray(dtype=tf.float32, size=dilation + self.num_decode_steps)\n            layer_ta = layer_ta.unstack(tf.transpose(slices, (1, 0, 2)))\n            state_queues.append(layer_ta)\n\n        # initialize feature tensor array\n        features_ta = tf.TensorArray(dtype=tf.float32, size=self.num_decode_steps)\n        features_ta = features_ta.unstack(tf.transpose(features, (1, 0, 2)))\n\n        # initialize output tensor array\n        emit_ta = tf.TensorArray(size=self.num_decode_steps, dtype=tf.float32)\n\n        # initialize other loop vars\n        elements_finished = 0 >= self.decode_len\n        time = tf.constant(0, dtype=tf.int32)\n\n        # get initial x input\n        current_idx = tf.stack([tf.range(tf.shape(self.encode_len)[0]), self.encode_len - 1], axis=1)\n        initial_input = tf.gather_nd(x, current_idx)\n\n        def loop_fn(time, current_input, queues):\n            current_features = features_ta.read(time)\n            current_input = tf.concat([current_input, current_features], axis=1)\n\n            with tf.variable_scope('x-proj-decode', reuse=True):\n                w_x_proj = tf.get_variable('weights')\n                b_x_proj = tf.get_variable('biases')\n                x_proj = tf.nn.tanh(tf.matmul(current_input, w_x_proj) + b_x_proj)\n\n            skip_outputs, updated_queues = [], []\n            for i, (conv_input, queue, dilation) in enumerate(zip(conv_inputs, queues, self.dilations)):\n\n                state = queue.read(time)\n                with tf.variable_scope('dilated-conv-decode-{}'.format(i), reuse=True):\n                    w_conv = tf.get_variable('weights'.format(i))\n                    b_conv = tf.get_variable('biases'.format(i))\n                    dilated_conv = tf.matmul(state, w_conv[0, :, :]) + tf.matmul(x_proj, w_conv[1, :, :]) + b_conv\n                conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=1)\n                dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n\n                with tf.variable_scope('dilated-conv-proj-decode-{}'.format(i), reuse=True):\n                    w_proj = tf.get_variable('weights'.format(i))\n                    b_proj = tf.get_variable('biases'.format(i))\n                    concat_outputs = tf.matmul(dilated_conv, w_proj) + b_proj\n                skips, residuals = tf.split(concat_outputs, [self.skip_channels, self.residual_channels], axis=1)\n\n                x_proj += residuals\n                skip_outputs.append(skips)\n                updated_queues.append(queue.write(time + dilation, x_proj))\n\n            skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=1))\n            with tf.variable_scope('dense-decode-1', reuse=True):\n                w_h = tf.get_variable('weights')\n                b_h = tf.get_variable('biases')\n                h = tf.nn.relu(tf.matmul(skip_outputs, w_h) + b_h)\n\n            with tf.variable_scope('dense-decode-2', reuse=True):\n                w_y = tf.get_variable('weights')\n                b_y = tf.get_variable('biases')\n                y_hat = tf.matmul(h, w_y) + b_y\n\n            elements_finished = (time >= self.decode_len)\n            finished = tf.reduce_all(elements_finished)\n\n            next_input = tf.cond(\n                finished,\n                lambda: tf.zeros([batch_size, 1], dtype=tf.float32),\n                lambda: y_hat\n            )\n            next_elements_finished = (time >= self.decode_len - 1)\n\n            return (next_elements_finished, next_input, updated_queues)\n\n        def condition(unused_time, elements_finished, *_):\n            return tf.logical_not(tf.reduce_all(elements_finished))\n\n        def body(time, elements_finished, emit_ta, *state_queues):\n            (next_finished, emit_output, state_queues) = loop_fn(time, initial_input, state_queues)\n\n            emit = tf.where(elements_finished, tf.zeros_like(emit_output), emit_output)\n            emit_ta = emit_ta.write(time, emit)\n\n            elements_finished = tf.logical_or(elements_finished, next_finished)\n            return [time + 1, elements_finished, emit_ta] + list(state_queues)\n\n        returned = tf.while_loop(\n            cond=condition,\n            body=body,\n            loop_vars=[time, elements_finished, emit_ta] + state_queues\n        )\n\n        outputs_ta = returned[2]\n        y_hat = tf.transpose(outputs_ta.stack(), (1, 0, 2))\n        return y_hat\n\n    def calculate_loss(self):\n        x = self.get_input_sequences()\n\n        y_hat_encode, conv_inputs = self.encode(x, features=self.encode_features)\n        self.initialize_decode_params(x, features=self.decode_features)\n        y_hat_decode = self.decode(y_hat_encode, conv_inputs, features=self.decode_features)\n        y_hat_decode = self.inverse_transform(tf.squeeze(y_hat_decode, 2))\n        y_hat_decode = tf.nn.relu(y_hat_decode)\n\n        self.labels = self.y_decode\n        self.preds = y_hat_decode\n        self.loss = sequence_smape(self.labels, self.preds, self.decode_len, self.is_nan_decode)\n\n        self.prediction_tensors = {\n            'priors': self.x_encode,\n            'labels': self.labels,\n            'preds': self.preds,\n            'page_id': self.page_id,\n        }\n\n        return self.loss\n\n\nif __name__ == '__main__':\n    base_dir = './'\n\n    dr = DataReader(data_dir=os.path.join(base_dir, 'data/processed/'))\n\n    nn = cnn(\n        reader=dr,\n        log_dir=os.path.join(base_dir, 'logs'),\n        checkpoint_dir=os.path.join(base_dir, 'checkpoints'),\n        prediction_dir=os.path.join(base_dir, 'predictions'),\n        optimizer='adam',\n        learning_rate=.001,\n        batch_size=128,\n        num_training_steps=200000,\n        early_stopping_steps=5000,\n        warm_start_init_step=0,\n        regularization_constant=0.0,\n        keep_prob=1.0,\n        enable_parameter_averaging=False,\n        num_restarts=2,\n        min_steps_to_checkpoint=500,\n        log_interval=10,\n        num_validation_batches=1,\n        grad_clip=20,\n        residual_channels=32,\n        skip_channels=32,\n        dilations=[2**i for i in range(8)]*3,\n        filter_widths=[2 for i in range(8)]*3,\n        num_decode_steps=64,\n    )\n    nn.fit()\n    nn.restore()\n    nn.predict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\nlengths_mat = np.load(os.path.join('predictions', 'lengths.npy'))\npreds_mat = np.load(os.path.join('predictions', 'preds.npy'))\nids_mat = np.load(os.path.join('predictions', 'ids.npy'))\npreds_mat[lengths_mat == 0] = 0\n\ndf = pd.DataFrame({'id': ids_mat.flatten(), 'unit_sales': preds_mat.flatten()})\ndf['unit_sales'] = df['unit_sales'].map(np.expm1)\n\ndf[['id', 'unit_sales']].to_csv('sub.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}